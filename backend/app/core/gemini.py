# app/core/llm.py
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from pydantic_settings import BaseSettings
from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda, RunnableBranch
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
from app.core.vector_db import collection, model

class Settings(BaseSettings):
    GEMINI_API_KEY: str

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        extra="ignore"

settings = Settings()

# Kh·ªüi t·∫°o LLM (Gemini) qua LangChain
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=settings.GEMINI_API_KEY,
    temperature=0.7,
    max_output_tokens=600,
)

classification_prompt = ChatPromptTemplate.from_template(
    """Ph√¢n lo·∫°i nhanh c√¢u h·ªèi sau c√≥ li√™n quan ƒë·∫øn du l·ªãch, vƒÉn h√≥a, ·∫©m th·ª±c, l·ªãch s·ª≠, ƒë·ªãa ƒëi·ªÉm t·∫°i Th·ª´a Thi√™n Hu·∫ø kh√¥ng?

C√¢u h·ªèi: {question}

Ch·ªâ tr·∫£ v·ªÅ ƒë√∫ng 1 t·ª´: YES ho·∫∑c NO
Kh√¥ng gi·∫£i th√≠ch g√¨ th√™m."""
)
main_prompt = ChatPromptTemplate.from_messages([
    ("system", """
B·∫°n l√† h∆∞·ªõng d·∫´n vi√™n du l·ªãch chuy√™n nghi·ªáp t·∫°i Th·ª´a Thi√™n Hu·∫ø, th√¢n thi·ªán, nhi·ªát t√¨nh, am hi·ªÉu s√¢u v·ªÅ l·ªãch s·ª≠, vƒÉn h√≥a, ·∫©m th·ª±c v√† c√°c ƒë·ªãa ƒëi·ªÉm.

**Quy t·∫Øc tr·∫£ l·ªùi nghi√™m ng·∫∑t:**
- ∆Øu ti√™n v√† ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ ph·∫ßn "Context li√™n quan" ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c, tr√°nh b·ªãa ƒë·∫∑t.
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn (3‚Äì8 c√¢u), t·ª± nhi√™n, g·∫ßn g≈©i nh∆∞ ƒëang tr√≤ chuy·ªán.
- N·∫øu kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p trong context ‚Üí tr·∫£ l·ªùi trung th·ª±c: "Hi·ªán t·∫°i m√¨nh ch∆∞a c√≥ th√¥ng tin chi ti·∫øt v·ªÅ ph·∫ßn n√†y ·∫°..."
- KH√îNG t·ª± g·ª£i √Ω th√™m ·ªü ƒë√¢y (g·ª£i √Ω s·∫Ω ƒë∆∞·ª£c th√™m ri√™ng ·ªü b∆∞·ªõc sau).
- Gi·ªØ gi·ªçng ƒëi·ªáu vui v·∫ª, ch√†o ƒë√≥n du kh√°ch.

Context li√™n quan (Ngu·ªìn ki·∫øn th·ª©c ch√≠nh t·ª´ RAG):
{context}

Hi·ªán t·∫°i ƒëang tr√≤ chuy·ªán v·ªõi: {username}
    """),
    ("human", "{question}")
])
suggestion_prompt = ChatPromptTemplate.from_messages([
    ("system", """
D·ª±a tr√™n c√¢u h·ªèi c·ªßa du kh√°ch: "{question}"

H√£y g·ª£i √Ω NG·∫ÆN G·ªåN (1‚Äì2 c√¢u) **m·ªôt** ƒë·ªãa ƒëi·ªÉm, m√≥n ƒÉn, ho·∫°t ƒë·ªông, ho·∫∑c tr·∫£i nghi·ªám li√™n quan t·∫°i Hu·∫ø ƒë·ªÉ h·ªç kh√°m ph√° th√™m.
V√≠ d·ª•:
- H·ªèi v·ªÅ lƒÉng t·∫©m ‚Üí g·ª£i √Ω ƒÉn c∆°m h·∫øn ho·∫∑c ch√πa Thi√™n M·ª• g·∫ßn ƒë√≥
- H·ªèi v·ªÅ ·∫©m th·ª±c ‚Üí g·ª£i √Ω ch·ª£ ƒê√¥ng Ba ho·∫∑c qu√°n b√∫n b√≤ Hu·∫ø n·ªïi ti·∫øng

Ch·ªâ tr·∫£ v·ªÅ n·ªôi dung g·ª£i √Ω, kh√¥ng n√≥i th√™m g√¨ kh√°c (kh√¥ng d√πng "T√¥i g·ª£i √Ω", "B·∫°n n√™n", ch·ªâ n·ªôi dung thu·∫ßn).
    """),
    ("human", "G·ª£i √Ω cho t√¥i nh√©!")
])
def retrieve_context_fn(inputs: dict) -> str:
    question = inputs.get("question", "")
    if not isinstance(question, str):
        return ""
    query = question.strip().lower()
    if not query:
        return ""
    try:
        query_emb = model.encode([query]).tolist()
        results = collection.query(
                query_embeddings=[query_emb],
                n_results=5
            )
        docs = results.get("documents", [[]])[0]
        return "\n".join(docs) if docs else ""
    except Exception as e:
            print(f"[VectorDB][ERROR] {e}")
            return "" # Tr·∫£ v·ªÅ "" t·ªët h∆°n v√¨ LLM c√≥ b·∫Øt l·ªói kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p context

retriever = RunnableLambda(retrieve_context_fn)
def format_output(x: dict) -> str:
    answer = x.get("answer", "").strip()
    suggestion = x.get("suggestion", "").strip() or "Hi·ªán ch∆∞a c√≥ g·ª£i √Ω ph√π h·ª£p ·∫°~"

    source = ""
    context = x.get("context", "")
    if context:
        ctx = context[:350].replace("\n", " ")
        source = f"\n\n(Ngu·ªìn tham kh·∫£o: {ctx}...)"

    return f"{answer}\n\n**G·ª£i √Ω th√™m:** {suggestion}{source}"
yes_branch = (
    RunnableParallel(
        # question=itemgetter("question"),
        # username=itemgetter("username"),
        # context=itemgetter("context"),
        # answer=main_prompt | llm | StrOutputParser(),
        # suggestion=suggestion_prompt | llm | StrOutputParser(),
        question=itemgetter("question"),
        username=itemgetter("username"),
        context=itemgetter("context"),
        answer=main_prompt | llm | StrOutputParser(),
        suggestion=RunnableLambda(lambda _: ""), #gi·∫£m request tr√°nh crash server/ keyfree.
    )
)
no_branch = RunnableLambda(
    lambda x: {
        "question": x["question"],
        "username": x["username"],
        "context": "",
        "answer": "M√¨nh hi·ªán ch·ªâ h·ªó tr·ª£ th√¥ng tin du l·ªãch t·∫°i Hu·∫ø th√¥i ·∫° üòä B·∫°n mu·ªën h·ªèi v·ªÅ ƒë·ªãa ƒëi·ªÉm, ·∫©m th·ª±c hay tr·∫£i nghi·ªám n√†o ·ªü Hu·∫ø kh√¥ng?",
        "suggestion": "",
    }
)


chain = (
    # B∆∞·ªõc 1: Nh·∫≠n input
    RunnableParallel(
        question=itemgetter("question"),  #bug dict != string
        username = itemgetter("username") #username=RunnableLambda(lambda x: x.get("username", "du kh√°ch")), ##ƒë·ªìng b·ªô v·ªõi question
    )

    # B∆∞·ªõc 2: L·∫•y context + ph√¢n lo·∫°i
    | RunnableParallel(
        question=itemgetter("question"),
        username=itemgetter("username"),
        context=retriever,
        is_hue=classification_prompt | llm | StrOutputParser(),
    )

    # B∆∞·ªõc 3: R·∫Ω nh√°nh theo ph√¢n lo·∫°i
    | RunnableBranch(
        (
            lambda x: x["is_hue"].strip().upper() == "YES",
            yes_branch,
        ),
        no_branch
    )

    # B∆∞·ªõc 4: Format output cu·ªëi
    | RunnableLambda(format_output)
)