# app/core/gemini.py
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from pydantic_settings import BaseSettings
from langchain_core.runnables import RunnableParallel, RunnableLambda, RunnableBranch
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
from app.core.vector_db import get_retriever

retriever = get_retriever()

class Settings(BaseSettings):
    GEMINI_API_KEY: str

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        extra="ignore"

settings = Settings()

# Khá»Ÿi táº¡o LLM (Gemini) qua LangChain
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=settings.GEMINI_API_KEY,
    temperature=0.7,
    max_output_tokens=600,
)

classification_prompt = ChatPromptTemplate.from_template(
    """PhÃ¢n loáº¡i nhanh cÃ¢u há»i sau cÃ³ liÃªn quan Ä‘áº¿n du lá»‹ch, vÄƒn hÃ³a, áº©m thá»±c, lá»‹ch sá»­, Ä‘á»‹a Ä‘iá»ƒm táº¡i Thá»«a ThiÃªn Huáº¿ khÃ´ng?

CÃ¢u há»i: {question}

Chá»‰ tráº£ vá» Ä‘Ãºng 1 tá»«: YES hoáº·c NO
KhÃ´ng giáº£i thÃ­ch gÃ¬ thÃªm."""
)
main_prompt = ChatPromptTemplate.from_messages([
    ("system", """
Báº¡n lÃ  hÆ°á»›ng dáº«n viÃªn du lá»‹ch chuyÃªn nghiá»‡p táº¡i Thá»«a ThiÃªn Huáº¿, thÃ¢n thiá»‡n, nhiá»‡t tÃ¬nh, am hiá»ƒu sÃ¢u vá» lá»‹ch sá»­, vÄƒn hÃ³a, áº©m thá»±c vÃ  cÃ¡c Ä‘á»‹a Ä‘iá»ƒm.

**Quy táº¯c tráº£ lá»i nghiÃªm ngáº·t:**
- Æ¯u tiÃªn vÃ  chá»‰ sá»­ dá»¥ng thÃ´ng tin tá»« pháº§n "Context liÃªn quan" Ä‘á»ƒ tráº£ lá»i chÃ­nh xÃ¡c, trÃ¡nh bá»‹a Ä‘áº·t.
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, ngáº¯n gá»n (3â€“8 cÃ¢u), tá»± nhiÃªn, gáº§n gÅ©i nhÆ° Ä‘ang trÃ² chuyá»‡n.
- Náº¿u khÃ´ng cÃ³ thÃ´ng tin phÃ¹ há»£p trong context â†’ tráº£ lá»i trung thá»±c: "Hiá»‡n táº¡i mÃ¬nh chÆ°a cÃ³ thÃ´ng tin chi tiáº¿t vá» pháº§n nÃ y áº¡..."
- KHÃ”NG tá»± gá»£i Ã½ thÃªm á»Ÿ Ä‘Ã¢y (gá»£i Ã½ sáº½ Ä‘Æ°á»£c thÃªm riÃªng á»Ÿ bÆ°á»›c sau).
- Giá»¯ giá»ng Ä‘iá»‡u vui váº», chÃ o Ä‘Ã³n du khÃ¡ch.

Context liÃªn quan (Nguá»“n kiáº¿n thá»©c chÃ­nh tá»« RAG):
{context}

Hiá»‡n táº¡i Ä‘ang trÃ² chuyá»‡n vá»›i: {username}
    """),
    ("human", "{question}")
])
suggestion_prompt = ChatPromptTemplate.from_messages([
    ("system", """
Dá»±a trÃªn cÃ¢u há»i cá»§a du khÃ¡ch: "{question}"

HÃ£y gá»£i Ã½ NGáº®N Gá»ŒN (1â€“2 cÃ¢u) **má»™t** Ä‘á»‹a Ä‘iá»ƒm, mÃ³n Äƒn, hoáº¡t Ä‘á»™ng, hoáº·c tráº£i nghiá»‡m liÃªn quan táº¡i Huáº¿ Ä‘á»ƒ há» khÃ¡m phÃ¡ thÃªm.
VÃ­ dá»¥:
- Há»i vá» lÄƒng táº©m â†’ gá»£i Ã½ Äƒn cÆ¡m háº¿n hoáº·c chÃ¹a ThiÃªn Má»¥ gáº§n Ä‘Ã³
- Há»i vá» áº©m thá»±c â†’ gá»£i Ã½ chá»£ ÄÃ´ng Ba hoáº·c quÃ¡n bÃºn bÃ² Huáº¿ ná»•i tiáº¿ng

Chá»‰ tráº£ vá» ná»™i dung gá»£i Ã½, khÃ´ng nÃ³i thÃªm gÃ¬ khÃ¡c (khÃ´ng dÃ¹ng "TÃ´i gá»£i Ã½", "Báº¡n nÃªn", chá»‰ ná»™i dung thuáº§n).
    """),
    ("human", "Gá»£i Ã½ cho tÃ´i nhÃ©!")
])
def retrieve_context_fn(inputs):
    question = inputs.get("question", "")
    if not isinstance(question, str) or not question.strip():
        return {**inputs, "context": ""}

    docs = retriever.invoke(question)

    context_text = "\n\n".join(
        f"[Nguá»“n: {doc.metadata.get('source', 'hue_knowledge.txt')}] {doc.page_content}"
        for doc in docs
    ) if docs else ""

    return {
        **inputs,
        "context": context_text
    }

chain = (
    #Chuáº©n hÃ³a input
    RunnableParallel(
        question=itemgetter("question"),
        username=itemgetter("username")
    )
    
    #Retrieve + PhÃ¢n loáº¡i
    | RunnableParallel(
        question=itemgetter("question"),
        username=itemgetter("username"),
        context=RunnableLambda(retrieve_context_fn) | itemgetter("context"),
        is_hue=classification_prompt | llm | StrOutputParser()
    )
    
    #Ráº½ nhÃ¡nh YES/NO
    | RunnableBranch(
        #YES: cÃ³ context Huáº¿ â†’ tráº£ lá»i
        (
            lambda x: "YES" in x["is_hue"].upper(),
            RunnableParallel(
                question=itemgetter("question"),
                username=itemgetter("username"),
                context=itemgetter("context"),
                answer=main_prompt | llm | StrOutputParser(),
                #suggestion=suggestion_prompt | llm | StrOutputParser()   ## giáº£m request/ freekey
            )
        ),
        #NO
        RunnableLambda(lambda x: {
            "question": x["question"],
            "username": x["username"],
            "context": "",
            "answer": "MÃ¬nh hiá»‡n chá»‰ há»— trá»£ thÃ´ng tin du lá»‹ch táº¡i Huáº¿ thÃ´i áº¡ ğŸ˜Š Báº¡n muá»‘n há»i vá» Ä‘á»‹a Ä‘iá»ƒm, áº©m thá»±c hay tráº£i nghiá»‡m nÃ o á»Ÿ Huáº¿ khÃ´ng?",
            "suggestion": ""
        })
    )
    
    #Format output
    | RunnableLambda(lambda x: {
        "answer": x["answer"].strip(),
        "suggestion": (x.get("suggestion") or "").strip(),
        "context": x["context"]
    })
    | RunnableLambda(lambda x: f"{x['answer']}\n\n**Gá»£i Ã½ thÃªm:** {x['suggestion']}" if x['suggestion'] else x['answer'])
)